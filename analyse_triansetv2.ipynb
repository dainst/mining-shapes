{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('tensorflow2gpu': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "45cb965d259566d63a3e50626ed20823c3a963e454ecba058204a235f5436453"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "import tensorflow as tf\r\n",
    "import os\r\n",
    "from pathlib import Path\r\n",
    "import datetime\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "#import Keras\r\n",
    "#from datumaro.components.project import Project\r\n",
    "from datumaro.components.dataset import Dataset\r\n",
    "import datumaro.plugins.transforms as transforms\r\n",
    "from datumaro.components.project import Environment, Project\r\n",
    "from datumaro.components.operations import merge_categories, MergingStrategy\r\n",
    "from datumaro.components.operations import IntersectMerge\r\n",
    "from datumaro.components.extractor import (Importer, Extractor, Transform, DatasetItem, Bbox, AnnotationType, Label,\r\n",
    "    LabelCategories, PointsCategories, MaskCategories)\r\n",
    "import zipfile\r\n",
    "import shutil\r\n",
    "#import random\r\n",
    "#from sklearn.model_selection import train_test_split\r\n",
    "#print(tf.version.VERSION)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "DIR = \"E:/Traindata/Trainingdata_fromCVAT/mining_figures/All\"\r\n",
    "TRAIN_PART = 0.7\r\n",
    "\r\n",
    "def provide_recorddf(path) -> pd.DataFrame:\r\n",
    "    list_of_files = []\r\n",
    "    for (dirpath, dirnames, filenames) in os.walk(path):\r\n",
    "        for filename in filenames:\r\n",
    "            row = {}\r\n",
    "            if filename in ['test.tfrecord', 'val.tfrecord', 'train.tfrecord']:\r\n",
    "                row['filename'] = filename\r\n",
    "                row['filepath'] = os.path.join(dirpath, filename)\r\n",
    "                list_of_files.append(row)\r\n",
    "                print(row)\r\n",
    "    return pd.DataFrame(list_of_files)\r\n",
    "\r\n",
    "def unzip_tfrecords(path):\r\n",
    "    i = 1\r\n",
    "    listoftfrecordfiles = []\r\n",
    "    for file in os.listdir(path):\r\n",
    "        if file.endswith('.zip'):\r\n",
    "\r\n",
    "                listoftfrecordfiles.append(tfrecordfiles)\r\n",
    "                #zip_ref.extractall(DIR)\r\n",
    "                i = i + 1\r\n",
    "    return listoftfrecordfiles\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def dataset_shapes(dataset):\r\n",
    "    try:\r\n",
    "        return [x.get_shape().as_list() for x in dataset._tensors]\r\n",
    "    except TypeError:\r\n",
    "        return dataset._tensors.get_shape().as_list()\r\n",
    "\r\n",
    "def loadtfrecord(path):\r\n",
    "\r\n",
    "    dataset = tf.data.TFRecordDataset(path, compression_type=None, buffer_size=None, num_parallel_reads=None)\r\n",
    "    return dataset\r\n",
    "\r\n",
    "def converttolist(path):\r\n",
    "    records = []\r\n",
    "    for record in tf.data.Iterator(path):\r\n",
    "        records.append(record)\r\n",
    "    return records\r\n",
    "\r\n",
    "def traintestsplit(dataset):\r\n",
    "    split = 3\r\n",
    "    dataset_train = dataset.window(split, split + 1).flat_map(lambda ds: ds)\r\n",
    "    dataset_test = dataset.skip(split).window(1, split + 1).flat_map(lambda ds: ds)\r\n",
    "    return dataset_train , dataset_test\r\n",
    "\r\n",
    "def writetrainandtest(train,test, i):\r\n",
    "    test_writer = os.path.join(DIR, 'x0' +str(i) + \"_test.tfrecord\")\r\n",
    "    train_writer = os.path.join(DIR, 'x0' + str(i) + \"_train.tfrecord\")\r\n",
    "\r\n",
    "    writer = tf.data.experimental.TFRecordWriter(test_writer)\r\n",
    "    writer.write(test)\r\n",
    "    writer = tf.data.experimental.TFRecordWriter(train_writer)\r\n",
    "    writer.write(train)\r\n",
    "def extract_fn(data_record):\r\n",
    "    features = {\r\n",
    "        # Extract features using the keys set during creation\r\n",
    "        \"image/class/label\":    tf.FixedLenFeature([], tf.int64),\r\n",
    "        \"image/encoded\":        tf.VarLenFeature(tf.string),\r\n",
    "    }\r\n",
    "    sample = tf.parse_single_example(data_record, features)\r\n",
    "    label = sample['image/class/label']\r\n",
    "    dense = tf.sparse_tensor_to_dense(sample['image/encoded'])\r\n",
    "\r\n",
    "    # Comment it if you got an error and inspect just dense:\r\n",
    "    image = tf.image.decode_image(dense, dtype=tf.float32) \r\n",
    "\r\n",
    "    return dense, image, label\r\n",
    "def correctionsMiningPages(listoftrainsets,listoftestsets,listofvalsets):\r\n",
    "    merger = IntersectMerge()\r\n",
    "    merged_trainset = merger(listoftrainsets)\r\n",
    "    del listoftrainsets\r\n",
    "    merged_trainset = merged_trainset.transform('remap_labels', {'stampbox': 'infoframe' }, default='keep')\r\n",
    "    merged_trainset = merged_trainset.transform('remap_labels', {'pageid': 'figureid' }, default='keep')\r\n",
    "    trainset_path = os.path.join(DIR, 'trainset.tfrecord')\r\n",
    "    print('trainset')\r\n",
    "    print(merged_trainset.categories())\r\n",
    "    print(len(merged_trainset))\r\n",
    "    merged_trainset.export(trainset_path, 'tf_detection_api', save_images=True)\r\n",
    "    merged_testset = merger(listoftestsets)\r\n",
    "    del listoftestsets\r\n",
    "\r\n",
    "    merged_testset = merged_testset.transform('remap_labels', {'stampbox': 'infoframe' }, default='keep')\r\n",
    "    \r\n",
    "    merged_testset = merged_testset.transform('remap_labels', {'pageid': 'figureid' }, default='keep')\r\n",
    "    testset_path = os.path.join(DIR, 'testset.tfrecord')\r\n",
    "    print('testset')\r\n",
    "    print(merged_testset.categories())\r\n",
    "    print(len(merged_testset))\r\n",
    "    merged_testset.export(testset_path, 'tf_detection_api', save_images=True)\r\n",
    "    merged_valset = merger(listofvalsets)\r\n",
    "    del listofvalsets\r\n",
    "\r\n",
    "    merged_valset = merged_valset.transform('remap_labels', {'stampbox': 'infoframe' }, default='keep')\r\n",
    "    merged_valset = merged_valset.transform('remap_labels', {'pageid': 'figureid' }, default='keep')\r\n",
    "    valset_path = os.path.join(DIR, 'valset.tfrecord')\r\n",
    "    print('valset')\r\n",
    "    print(merged_valset.categories())\r\n",
    "    print(len(merged_valset))\r\n",
    "    merged_valset.export(valset_path, 'tf_detection_api', save_images=True)\r\n",
    "\r\n",
    "def splitEachRecord(listoftfrecordfiles):\r\n",
    "    listoftrainsets = []\r\n",
    "    listoftestsets = []\r\n",
    "    listofvalsets = []\r\n",
    "    for record in listoftfrecordfiles:\r\n",
    "        print(record['name'])\r\n",
    "        \r\n",
    "        \r\n",
    "        dataset= Dataset.import_from(record['tfrpath'], 'tf_detection_api')\r\n",
    "        #cleanset = dataset.select(lambda item: len(item.annotations) <= 2)\r\n",
    "        #for item in cleanset:\r\n",
    "            #print(item.annotations)\r\n",
    "\r\n",
    "\r\n",
    "        print(record['tfrpath'])\r\n",
    "        if 'task_mining_figures_zenonid_000066595_300pages-2021_02_16_12_47_10-tfrecord 1.0' in record['name']:\r\n",
    "            print(len(dataset))\r\n",
    "            dataset=dataset.select(lambda item: len(item.annotations) != 0)\r\n",
    "            print(len(dataset))\r\n",
    "        if 'task_mining_pages_zenonid_000147534_selectedpages-2021_02_23_16_00_02-tfrecord 1.0' in record['name']:\r\n",
    "            dataset = dataset.transform('remap_labels', {'stampfigure':'stampfigure','vesselprofilefigure': 'vesselprofilefigure', 'pageid':'pageid', 'pageinfo':'pageinfo', 'vesselimage':'vesselimage' }, default='delete')\r\n",
    "        if 'task_mining_pages_zenonid_000009465-2020_11_10_09_30_39-tfrecord 1.0' in record['name']:\r\n",
    "            dataset = dataset.transform('remap_labels', {'stampfigure':'stampfigure','vesselprofilefigure': 'vesselprofilefigure', 'pageid':'pageid', 'pageinfo':'pageinfo', 'vesselimage':'vesselimage' }, default='delete')\r\n",
    "        if 'task_zenonid_000267012_and_zenonid_001508696-2020_11_20_09_34_56-tfrecord 1.0' in record['name']:\r\n",
    "            dataset = dataset.transform('remap_labels', {'stampfigure':'stampfigure','vesselprofilefigure': 'vesselprofilefigure', 'pageid':'pageid', 'pageinfo':'pageinfo', 'vesselimage':'vesselimage' }, default='delete')\r\n",
    "        if 'task_zenonid_001344933_and_zenonid_001346932-2020_11_12_14_13_46-tfrecord 1.0' in record['name']:\r\n",
    "            dataset = dataset.transform('remap_labels', {'stampfigure':'stampfigure','vesselprofilefigure': 'vesselprofilefigure', 'pageid':'pageid', 'pageinfo':'pageinfo', 'vesselimage':'vesselimage' }, default='delete')\r\n",
    "\r\n",
    "        splitted = transforms.RandomSplit(dataset, splits=[('train', 0.60), ('test', 0.15), ('val', 0.25)])\r\n",
    "        train = splitted.get_subset('train')\r\n",
    "        trainset = Dataset.from_extractors(train)\r\n",
    "        test = splitted.get_subset('test')\r\n",
    "        testset = Dataset.from_extractors(test)\r\n",
    "        val = splitted.get_subset('val')\r\n",
    "        valset = Dataset.from_extractors(val)\r\n",
    "        #train, test = splitter\r\n",
    "        listoftrainsets.append(trainset)\r\n",
    "        listoftestsets.append(testset)\r\n",
    "        listofvalsets.append(valset)\r\n",
    "\r\n",
    "    return listoftrainsets,listoftestsets,listofvalsets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "listofrecords = provide_recorddf(DIR) \r\n",
    "print(listofrecords.iloc[0]['filepath'])\r\n",
    "onimageList = []\r\n",
    "annotationList = []\r\n",
    "for index,record in listofrecords.iterrows():\r\n",
    "    dataset= Dataset.import_from(record['filepath'], 'tf_detection_api')\r\n",
    "    for data in dataset:\r\n",
    "        dict = vars(data)\r\n",
    "        onimageList.append(dict)\r\n",
    "        for anno in dict['annotations']:\r\n",
    "            \r\n",
    "            annodict= vars(anno)\r\n",
    "            #annodictbig = {**dict, **annodict}\r\n",
    "            annotationList.append(annodictbig)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "annotationDF = pd.DataFrame(annotationList)\r\n",
    "print(len(onimageList))\r\n",
    "print(onimageList[105])\r\n",
    "print(annotationDF.columns)\r\n",
    "pathgroups = annotationDF['id']\r\n",
    "print(pathgroups)\r\n",
    "for name, group in  annotationDF.groupby('label'):\r\n",
    "    print(name, len(group))\r\n",
    "#print(len(annotationList))\r\n",
    "    #print(record['filepath'], len(dataset))\r\n",
    "#dataset = Environment().make_importer('E:/Traindata/Trainingdata_fromCVAT/mining_figures/Bonifay2004quick/testset.tfrecord').make_dataset()\r\n",
    "# load a Datumaro project\r\n",
    "#project = Project.load('E:/Traindata/Trainingdata_fromCVAT/mining_pages/datumaroproject')\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "#dataset = Dataset.from_extractors(dataset1, dataset2)\r\n",
    "#ms = MergingStrategy()\r\n",
    "\r\n",
    "#print(merged.categories())\r\n",
    "#print(dataset2.categories())\r\n",
    "#mergecats = ms.merge([dataset1.categories(), dataset2.categories()])\r\n",
    "#categories = dataset.transform.categories()\r\n",
    "#print(categories.get(AnnotationType.label))\r\n",
    "#newdataset = dataset.transform('remap_labels', {'pageid': 'dog' }, default='delete')\r\n",
    "#print(mergecats)\r\n",
    "#for key, value in categories.items() :\r\n",
    "    #print(type(categories[key]))\r\n",
    "#print(dataset[0])\r\n",
    "#for item in dataset:\r\n",
    "    #print(item)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "  #print(item.annotations)\r\n",
    "#dataset.export(cocofile, 'coco')\r\n",
    "# create a dataset\r\n",
    "#dataset = project.make_dataset()\r\n",
    "\r\n",
    "# keep only annotated images\r\n",
    "#dataset.select(lambda item: len(item.annotations) != 0)\r\n",
    "#print(dataset.categories())\r\n",
    "\r\n",
    "# change dataset labels\r\n",
    "#dataset.transform('remap_labels', {'0': '666'}, default = 'delete')\r\n",
    "\r\n",
    "# iterate over dataset elements\r\n",
    "#i = 0\r\n",
    "#for item in dataset:\r\n",
    "    #if i < 20:\r\n",
    "        #for a in item.annotations:\r\n",
    "            #print(a.label)\r\n",
    "    #i = i +1 \r\n",
    "\r\n",
    "#{<AnnotationType.label: 1>: LabelCategories(attributes=set(), items=[LabelCategories.Category(name='pageid', parent='', attributes=set()), LabelCategories.Category(name='pageinfo', parent='', attributes=set()), LabelCategories.Category(name='vesselprofilefigure', parent='', attributes=set()), LabelCategories.Category(name='vesselimage', parent='', attributes=set()), LabelCategories.Category(name='infoframe', parent='', attributes=set()), LabelCategories.Category(name='stampfigure', parent='', attributes=set())], _indices={'pageid': 0, 'pageinfo': 1, 'vesselprofilefigure': 2, 'vesselimage': 3, 'infoframe': 4, 'stampfigure': 5})}\r\n",
    "    #for i in item.annotations:\r\n",
    "        #print(i)\r\n",
    "  #print(item.id, item.annotations)\r\n",
    "\r\n",
    "# export the resulting dataset in COCO format\r\n",
    "#dataset.export('dst/dir', 'coco')\r\n",
    "#dataset = tf.data.TFRecordDataset(filename, compression_type=None, buffer_size=None, num_parallel_reads=None)\r\n",
    "#for example in tf.compat.v1.python_io.tf_record_iterator(filename):\r\n",
    "    #print(tf.train.Example.FromString(example))\r\n",
    "#for element in dataset:\r\n",
    "    #print(elemen)\r\n",
    "#print(list(dataset.as_numpy_iterator()))\r\n",
    "\r\n",
    "#dataset = dataset.map(extract_fn)\r\n",
    "#iterator = dataset.make_one_shot_iterator()\r\n",
    "#next_element = iterator.get_next()\r\n",
    "\r\n",
    "\r\n",
    "#tf.enable_eager_execution()\r\n",
    "#for images, labels in dataset.take(1):  # only take first element of dataset\r\n",
    "    #numpy_images = images.numpy()\r\n",
    "    #numpy_labels = labels.numpy()\r\n",
    "    #print(numpy_labels)\r\n",
    "    #image = image.reshape(IMAGE_SHAPE)\r\n",
    "\r\n",
    "\r\n",
    "#for tfrecords in listoftfrecordfiles:\r\n",
    "    #dataset=loadtfrecord(tfrecords['tfrpath'])\r\n",
    "    #for element in dataset.as_numpy_iterator():\r\n",
    "\r\n",
    "    #train, test = traintestsplit(record)\r\n",
    "    #writetrainandtest(train,test, tfrecords['id'])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "17916\n",
      "{'id': '08a624db-d4ec-426c-b37a-4839ef4d7ff6', 'annotations': [Bbox(id=0, attributes={}, group=0, points=[560.03, 35.91, 681.58, 81.93], label=3, z_order=0), Bbox(id=0, attributes={}, group=0, points=[10.49, 41.11, 59.63, 84.66], label=0, z_order=0)], 'subset': 'test', 'path': [], 'image': <datumaro.util.image.ByteImage object at 0x000001F1538D99C8>, 'point_cloud': None, 'related_images': [], 'attributes': {'source_id': ''}}\n",
      "Index(['id', 'annotations', 'subset', 'path', 'image', 'point_cloud',\n",
      "       'related_images', 'attributes', 'group', 'points', 'label', 'z_order'],\n",
      "      dtype='object')\n",
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "21757    0\n",
      "21758    0\n",
      "21759    0\n",
      "21760    0\n",
      "21761    0\n",
      "Name: id, Length: 21762, dtype: int64\n",
      "0 11977\n",
      "1 2841\n",
      "2 3030\n",
      "3 3914\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}