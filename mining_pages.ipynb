{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599558715740",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pdf2image'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6e3cd96091ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muuid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmining_pages_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_ocr_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_page\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mocr_pre_processing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut_image_savetemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut_image_figid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpdf_to_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmining_pages_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_page_labelmap_as_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_figid_labelmap_as_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_page_detections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_detections_figureidv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmining_pages_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfilter_best_page_detections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_best_vesselprofile_detections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprovide_pagelist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/Code/mining_pages_utils/image_ocr_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muuid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpdf2image\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_from_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpdf_to_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pdf2image'"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from distutils.version import StrictVersion\n",
    "import pytesseract\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "from mining_pages_utils.image_ocr_utils import load_page, cut_image, ocr_pre_processing, cut_image_savetemp, cut_image_figid, pdf_to_image\n",
    "from mining_pages_utils.dataframe_utils import get_page_labelmap_as_df, get_figid_labelmap_as_df, extract_page_detections, extract_detections_figureidv2\n",
    "from mining_pages_utils.dataframe_utils import filter_best_page_detections, filter_best_vesselprofile_detections, merge_info, split, provide_pagelist\n",
    "from mining_pages_utils.json_utils import create_find_JSONL, create_type_JSONL, create_drawing_JSONL, create_catalog_JSONL, create_trench_JSONL\n",
    "from mining_pages_utils.tensorflow_utils import create_tf_example, create_tf_figid, run_inference_for_page_series, run_inference_for_figure_series, run_vesselprofile_segmentation\n",
    "\n",
    "\n",
    "INPUTDIRECTORY = '/home/images/apply'\n",
    "GRAPH = '/frozen_inference_graph.pb'\n",
    "LABELS = '/label_map.pbtxt'\n",
    "PAGE_MODEL = '/home/models/inference_graph_mining_pages_v8'\n",
    "FIGID_MODEL = '/home/models/inference_graph_figureid_v1'\n",
    "SEG_MODEL = '/home/models/shape_segmentation/train_colab_20200610.h5'\n",
    "OUTPATH = '/home/images/OUTPUT/'\n",
    "VESSELLPATH = OUTPATH + 'vesselprofiles/'\n",
    "SEGMENTPATH = OUTPATH + 'segmented_profiles/'\n",
    "CSVOUT = OUTPATH + 'mining_pages_allinfo.csv'\n",
    "\n",
    "classlist = ['pageid', 'pageinfo']\n",
    "figureclasslist = ['vesselprofilefigure']\n",
    "figureidclasslist = ['figureid']\n",
    "pageid_config = r'--psm 6 -c load_system_dawg=0 load_freq_dawg=0'\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdflist = provide_pdf_path(INPUTDIRECTORY)\n",
    "for pdf_path in pdflist:\n",
    "    pdf_to_image(pdf_path)\n",
    "pagelist = provide_pagelist(INPUTDIRECTORY)\n",
    "\n",
    "\n",
    "\n",
    "if StrictVersion(tf.version.VERSION) < StrictVersion('1.9.0'):\n",
    "    raise ImportError(\n",
    "        'Please upgrade your TensorFlow installation to v1.9.* or later!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Perform image segmentation\n396/396 [==============================] - 262s 660ms/step\n"
    }
   ],
   "source": [
    "for path in [VESSELLPATH, SEGMENTPATH]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.io.gfile.GFile(PAGE_MODEL + GRAPH, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "\n",
    "with detection_graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        # Get handles to input and output tensors\n",
    "        ops = tf.get_default_graph().get_operations()\n",
    "        all_tensor_names = {\n",
    "            output.name for op in ops for output in op.outputs}\n",
    "        tensor_dict = {}\n",
    "        for key in [\n",
    "            'num_detections', 'detection_boxes', 'detection_scores',\n",
    "            'detection_classes', 'detection_masks'\n",
    "        ]:\n",
    "            tensor_name = key + ':0'\n",
    "            if tensor_name in all_tensor_names:\n",
    "                tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "                    tensor_name)\n",
    "\n",
    "        all_detections_step1 = pd.DataFrame()\n",
    "\n",
    "        for index, row in pagelist.iterrows():\n",
    "\n",
    "            img = load_page(row)\n",
    "            result = run_inference_for_page_series(img, tensor_dict, sess)\n",
    "            result.drop(\"page_imgnp\", inplace=True)\n",
    "            all_detections_step1 = all_detections_step1.append(result)\n",
    "\n",
    "\n",
    "all_detections_step2 = extract_page_detections(\n",
    "    all_detections_step1, category_index=get_page_labelmap_as_df(PAGE_MODEL + LABELS))\n",
    "\n",
    "bestpages = filter_best_page_detections(all_detections_step2, classlist, lowest_score=0.7)\n",
    "pageid_raw = pd.DataFrame()\n",
    "\n",
    "#perform ocr page number\n",
    "for index, row in bestpages.iterrows():\n",
    "    img = cut_image(row)\n",
    "    img2 = ocr_pre_processing(img)\n",
    "    result = pytesseract.image_to_string(img2, config=pageid_config)\n",
    "    row['newinfo'] = result\n",
    "    pageid_raw = pageid_raw.append(row)\n",
    "\n",
    "all_detections_step3 = merge_info(all_detections_step2, pageid_raw)\n",
    "figures = filter_best_vesselprofile_detections(all_detections_step3, figureclasslist,lowest_score= 0.7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detect figure id\n",
    "detection_figureid_graph = tf.Graph()\n",
    "with detection_figureid_graph.as_default():\n",
    "    od_graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.io.gfile.GFile(FIGID_MODEL + GRAPH, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "\n",
    "with detection_figureid_graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        # Get handles to input and output tensors\n",
    "        ops = tf.get_default_graph().get_operations()\n",
    "        all_tensor_names = {\n",
    "            output.name for op in ops for output in op.outputs}\n",
    "\n",
    "        tensor_dict = {}\n",
    "        for key in [\n",
    "            'num_detections', 'detection_boxes', 'detection_scores',\n",
    "            'detection_classes', 'detection_masks'\n",
    "        ]:\n",
    "            tensor_name = key + ':0'\n",
    "            if tensor_name in all_tensor_names:\n",
    "                tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "                    tensor_name)\n",
    "\n",
    "        figures_step1 = pd.DataFrame()\n",
    "\n",
    "        for index, row in figures.iterrows():\n",
    "            img = cut_image_savetemp(row, VESSELLPATH)\n",
    "            result = run_inference_for_figure_series(\n",
    "                img, tensor_dict, sess)\n",
    "            result.drop(\"figure_imgnp\", inplace=True)\n",
    "            figures_step1 = figures_step1.append(result)\n",
    "\n",
    "\n",
    "figid_category_index = get_figid_labelmap_as_df(FIGID_MODEL + LABELS)\n",
    "figid_detections = figures_step1.apply(\n",
    "    extract_detections_figureidv2, axis=1)\n",
    "figures_step2 = figid_detections.merge(\n",
    "    figid_category_index, on=['figid_detection_classes'], how='left')\n",
    "\n",
    "#perform ocr figid\n",
    "figures_step3 = pd.DataFrame()\n",
    "for index, row in figures_step2.iterrows():\n",
    "    img = cut_image_figid(row)\n",
    "    img2 = ocr_pre_processing(img)\n",
    "    row['figid_raw'] = pytesseract.image_to_string(img2, config=pageid_config)\n",
    "    figures_step3 = figures_step3.append(row)\n",
    "\n",
    "\n",
    "with open(OUTPATH + 'catalogs.jsonl', 'w') as f:\n",
    "    pubs = figures_step3[['pub_key', 'pub_value']].drop_duplicates()\n",
    "    pubs.apply(create_catalog_JSONL, file=f, axis=1)\n",
    "with open(OUTPATH + 'trenches.jsonl', 'w') as f:\n",
    "    pubs = figures_step3[['pub_key', 'pub_value']].drop_duplicates()\n",
    "    pubs.apply(create_trench_JSONL, file=f, axis=1)\n",
    "with open(OUTPATH + 'types.jsonl', 'w') as f:\n",
    "    figures_step3.apply(create_type_JSONL, file=f, axis=1)\n",
    "with open(OUTPATH + 'finds.jsonl', 'w') as f:\n",
    "    figures_step3.apply(create_find_JSONL, file=f, axis=1)\n",
    "with open(OUTPATH + 'drawings.jsonl', 'w') as f:\n",
    "    figures_step3.apply(create_drawing_JSONL, file=f, axis=1)\n",
    "\n",
    "\n",
    "TFRECORDOUT = OUTPATH + 'mining_pages.tfrecord'\n",
    "writer = tf.io.TFRecordWriter(TFRECORDOUT)\n",
    "\n",
    "shutil.copyfile(PAGE_MODEL + LABELS, OUTPATH + 'pages_label_map.pbtxt')\n",
    "\n",
    "mining_pages_detections = figures_step3.append(bestpages)\n",
    "grouped = split(mining_pages_detections, 'page_path')\n",
    "\n",
    "for group in grouped:\n",
    "    tf_example = create_tf_example(group,  TFRECORDOUT)\n",
    "    writer.write(tf_example.SerializeToString())\n",
    "\n",
    "writer.close()\n",
    "\n",
    "TFRECORDOUT = OUTPATH + 'mining_figures.tfrecord'\n",
    "writer = tf.io.TFRecordWriter(TFRECORDOUT)\n",
    "\n",
    "shutil.copyfile(FIGID_MODEL + LABELS, OUTPATH + 'figures_label_map.pbtxt')\n",
    "figids = figures_step3[figures_step3.figid_detection_boxes.notnull()]\n",
    "\n",
    "figsgrouped = split(figids, 'figure_path')\n",
    "\n",
    "for group in figsgrouped:\n",
    "    figtf_example = create_tf_figid(group,  TFRECORDOUT)\n",
    "    writer.write(figtf_example.SerializeToString())\n",
    "\n",
    "writer.close()\n",
    "figures_step3.to_csv(CSVOUT)\n",
    "\n",
    "#Profile segmentation\n",
    "print('Perform image segmentation')\n",
    "run_vesselprofile_segmentation(VESSELLPATH, SEGMENTPATH, SEG_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}