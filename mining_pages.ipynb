{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.9 64-bit",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from distutils.version import StrictVersion\n",
    "import pytesseract\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import inflect\n",
    "import uuid\n",
    "\n",
    "from mining_pages_utils.image_ocr_utils import load_page, cut_image, ocr_pre_processing, ocr_post_processing_pageid, cut_image_savetemp, cut_image_figid, ocr_post_processing_figid\n",
    "from mining_pages_utils.dataframe_utils import get_page_labelmap_as_df, get_figid_labelmap_as_df, extract_page_detections, extract_page_detections_new,unfold_pagedetections, page_detections_toframe, extract_detections_figureidv2,humanreadID\n",
    "from mining_pages_utils.dataframe_utils import filter_best_page_detections, filter_best_vesselprofile_detections, merge_info, split, provide_pagelist, provide_pdf_path, get_pubs_and_configs, pdf_to_image, handleduplicate_humanreadID\n",
    "from mining_pages_utils.json_utils import create_find_JSONL, create_constructivisttype_JSONL, create_normativtype_JSONL, create_drawing_JSONL, create_catalog_JSONL, create_trench_JSONL\n",
    "from mining_pages_utils.tensorflow_utils import create_tf_example_new, create_tf_figid, run_inference_for_page_series, run_inference_for_figure_series\n",
    "\n",
    "\n",
    "if StrictVersion(tf.version.VERSION) < StrictVersion('1.9.0'):\n",
    "    raise ImportError(\n",
    "        'Please upgrade your TensorFlow installation to v1.9.* or later!')\n",
    "\n",
    "INPUTDIRECTORY = '/home/images/apply' \n",
    "GRAPH = '/frozen_inference_graph.pb'\n",
    "LABELS = '/label_map.pbtxt'\n",
    "PAGE_MODEL = '/home/models/inference_graph_mining_pages_v8'\n",
    "FIGID_MODEL = '/home/models/inference_graph_figureid_v1'\n",
    "SEG_MODEL = '/home/models/shape_segmentation/train_colab_20200610.h5'\n",
    "OUTPATH = '/home/images/OUTPUT/'\n",
    "VESSELLPATH = OUTPATH + 'vesselprofiles/'\n",
    "SEGMENTPATH = OUTPATH + 'segmented_profiles/'\n",
    "CSVOUT = OUTPATH + 'mining_pages_allinfo.csv'\n",
    "CLEANCSVOUT = OUTPATH + 'mining_pages_clean.csv'\n",
    "\n",
    "classlist = ['pageid', 'pageinfo']\n",
    "figureclasslist = ['vesselprofilefigure']\n",
    "figureidclasslist = ['figureid']\n",
    "pageid_config = r'--psm 6 -c load_system_dawg=0 load_freq_dawg=0'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "publist = get_pubs_and_configs(INPUTDIRECTORY)\n",
    "pdflist = provide_pdf_path(publist)\n",
    "pdflistv2 = pdflist.apply(pdf_to_image, axis=1)\n",
    "pagelist = provide_pagelist(pdflistv2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/images/apply/ZenonID_000255878/Ettlinger_1990-Conspectus_Formarum.pdf0002-063.png\n",
      "/home/images/apply/ZenonID_000255878/Ettlinger_1990-Conspectus_Formarum.pdf0002-064.png\n",
      "/home/images/apply/ZenonID_000255878/Ettlinger_1990-Conspectus_Formarum.pdf0002-065.png\n",
      "/home/images/apply/ZenonID_000255878/Ettlinger_1990-Conspectus_Formarum.pdf0002-066.png\n",
      "/home/images/apply/ZenonID_000255878/Ettlinger_1990-Conspectus_Formarum.pdf0002-067.png\n",
      "/home/images/apply/ZenonID_000255878/Ettlinger_1990-Conspectus_Formarum.pdf0002-068.png\n",
      "/home/images/apply/ZenonID_000255878/Ettlinger_1990-Conspectus_Formarum.pdf0002-069.png\n",
      "/home/images/apply/ZenonID_000255878/Ettlinger_1990-Conspectus_Formarum.pdf0002-070.png\n",
      "/home/images/apply/ZenonID_000255878/Ettlinger_1990-Conspectus_Formarum.pdf0002-071.png\n",
      "/home/images/apply/ZenonID_000255878/Ettlinger_1990-Conspectus_Formarum.pdf0002-072.png\n",
      "/home/images/apply/ZenonID_000255878/Ettlinger_1990-Conspectus_Formarum.pdf0002-073.png\n",
      "/home/images/apply/ZenonID_000255878/Ettlinger_1990-Conspectus_Formarum.pdf0002-074.png\n",
      "/home/images/apply/ZenonID_000255878/Ettlinger_1990-Conspectus_Formarum.pdf0003-121.png\n",
      "/home/images/apply/ZenonID_000255878/Ettlinger_1990-Conspectus_Formarum.pdf0003-122.png\n",
      "/home/images/apply/ZenonID_000255878/Ettlinger_1990-Conspectus_Formarum.pdf0003-123.png\n",
      "/home/images/apply/ZenonID_000255878/Ettlinger_1990-Conspectus_Formarum.pdf0003-124.png\n",
      "/home/images/apply/ZenonID_000255878/Ettlinger_1990-Conspectus_Formarum.pdf0003-125.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for path in [VESSELLPATH, SEGMENTPATH]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.io.gfile.GFile(PAGE_MODEL + GRAPH, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "\n",
    "with detection_graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        # Get handles to input and output tensors\n",
    "        ops = tf.get_default_graph().get_operations()\n",
    "        all_tensor_names = {\n",
    "            output.name for op in ops for output in op.outputs}\n",
    "        tensor_dict = {}\n",
    "        for key in [\n",
    "            'num_detections', 'detection_boxes', 'detection_scores',\n",
    "            'detection_classes', 'detection_masks'\n",
    "        ]:\n",
    "            tensor_name = key + ':0'\n",
    "            if tensor_name in all_tensor_names:\n",
    "                tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "                    tensor_name)\n",
    "\n",
    "        all_detections_step1 = pd.DataFrame()\n",
    "\n",
    "        for index, row in pagelist.iterrows():\n",
    "\n",
    "            img = load_page(row)\n",
    "            result = run_inference_for_page_series(img, tensor_dict, sess)\n",
    "            del img\n",
    "            result.drop(\"page_imgnp\", inplace=True)\n",
    "            all_detections_step1 = all_detections_step1.append(result)\n",
    "\n",
    "#all_detections_step2, keylist =unfold_pagedetections(all_detections_step1)\n",
    "#all_detections_step22 = extract_page_detections(all_detections_step2, keylist, category_index=get_page_labelmap_as_df(PAGE_MODEL + LABELS))\n",
    "all_detections_step2 = page_detections_toframe(all_detections_step1)\n",
    "page_category_index = get_page_labelmap_as_df(PAGE_MODEL + LABELS)\n",
    "all_detections_step2 = all_detections_step2.merge(page_category_index, on=['detection_classes'], how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['pageid' 'pageinfo']\n"
     ]
    }
   ],
   "source": [
    "bestpages = filter_best_page_detections(all_detections_step2, classlist, lowest_score=0.7)\n",
    "pageid_raw = pd.DataFrame()\n",
    "\n",
    "#perform ocr page number\n",
    "for index, row in bestpages.iterrows():\n",
    "    img = cut_image(row)\n",
    "    img2 = ocr_pre_processing(img)\n",
    "    result = pytesseract.image_to_string(img2, config=pageid_config)\n",
    "    row['newinfo'] = result\n",
    "    pageid_raw = pageid_raw.append(row)\n",
    "all_detections_step3 = merge_info(all_detections_step2, pageid_raw)\n",
    "all_detections_step3 = all_detections_step3.apply(ocr_post_processing_pageid, axis=1)\n",
    "\n",
    "figures = filter_best_vesselprofile_detections(all_detections_step3, figureclasslist,lowest_score= 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'radd'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-61edc7b9651c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mfigures_step3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfigures_step3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mocr_post_processing_figid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mfigures_step3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfigures_step3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhumanreadID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mfigures_step3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandleduplicate_humanreadID\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigures_step3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/Code/mining_pages_utils/dataframe_utils.py\u001b[0m in \u001b[0;36mhandleduplicate_humanreadID\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhandleduplicate_humanreadID\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'HRID'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HRID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mradd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_dupID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'radd'"
     ]
    }
   ],
   "source": [
    "#detect figure id\n",
    "detection_figureid_graph = tf.Graph()\n",
    "with detection_figureid_graph.as_default():\n",
    "    od_graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.io.gfile.GFile(FIGID_MODEL + GRAPH, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "\n",
    "with detection_figureid_graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        # Get handles to input and output tensors\n",
    "        ops = tf.get_default_graph().get_operations()\n",
    "        all_tensor_names = {\n",
    "            output.name for op in ops for output in op.outputs}\n",
    "\n",
    "        tensor_dict = {}\n",
    "        for key in [\n",
    "            'num_detections', 'detection_boxes', 'detection_scores',\n",
    "            'detection_classes', 'detection_masks'\n",
    "        ]:\n",
    "            tensor_name = key + ':0'\n",
    "            if tensor_name in all_tensor_names:\n",
    "                tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "                    tensor_name)\n",
    "\n",
    "        figures_step1 = pd.DataFrame()\n",
    "\n",
    "        for index, row in figures.iterrows():\n",
    "            img = cut_image_savetemp(row, VESSELLPATH)\n",
    "            result = run_inference_for_figure_series(\n",
    "                img, tensor_dict, sess)\n",
    "            result.drop(\"figure_imgnp\", inplace=True)\n",
    "            figures_step1 = figures_step1.append(result)\n",
    "\n",
    "\n",
    "figid_category_index = get_figid_labelmap_as_df(FIGID_MODEL + LABELS)\n",
    "figid_detections = figures_step1.apply(\n",
    "    extract_detections_figureidv2, axis=1)\n",
    "figures_step2 = figid_detections.merge(\n",
    "    figid_category_index, on=['figid_detection_classes'], how='left')\n",
    "\n",
    "#perform ocr figid\n",
    "figures_step3 = pd.DataFrame()\n",
    "for index, row in figures_step2.iterrows():\n",
    "    img = cut_image_figid(row)\n",
    "    img2 = ocr_pre_processing(img)\n",
    "    row['figid_raw'] = pytesseract.image_to_string(img2, config=pageid_config)\n",
    "    figures_step3 = figures_step3.append(row)\n",
    "figures_step3 = figures_step3.apply(ocr_post_processing_figid, axis=1)\n",
    "figures_step3 = figures_step3.apply(humanreadID, axis=1)\n",
    "figures_step3 = handleduplicate_humanreadID(figures_step3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(OUTPATH + 'catalogs.jsonl', 'w') as f:\n",
    "    pubs = figures_step3[['pub_key', 'pub_value']].drop_duplicates()\n",
    "    pubs.apply(create_catalog_JSONL, file=f, axis=1)\n",
    "with open(OUTPATH + 'trenches.jsonl', 'w') as f:\n",
    "    pubs = figures_step3[['pub_key', 'pub_value']].drop_duplicates()\n",
    "    pubs.apply(create_trench_JSONL, file=f, axis=1)\n",
    "with open(OUTPATH + 'types.jsonl', 'w') as f:\n",
    "    figures_step3.apply(create_constructivisttype_JSONL, file=f, axis=1)\n",
    "with open(OUTPATH + 'types_standalone.jsonl', 'w') as f:\n",
    "    figures_step3.apply(create_normativtype_JSONL, file=f, axis=1)\n",
    "with open(OUTPATH + 'finds.jsonl', 'w') as f:\n",
    "    figures_step3.apply(create_find_JSONL, file=f, axis=1)\n",
    "with open(OUTPATH + 'drawings.jsonl', 'w') as f:\n",
    "    figures_step3.apply(create_drawing_JSONL, file=f, axis=1)\n",
    "\n",
    "\n",
    "TFRECORDOUT = OUTPATH + 'mining_pages.tfrecord'\n",
    "writer = tf.io.TFRecordWriter(TFRECORDOUT)\n",
    "\n",
    "shutil.copyfile(PAGE_MODEL + LABELS, OUTPATH + 'pages_label_map.pbtxt')\n",
    "\n",
    "mining_pages_detections = figures_step3.append(bestpages)\n",
    "grouped = split(mining_pages_detections, 'page_path')\n",
    "\n",
    "for group in grouped:\n",
    "    tf_example = create_tf_example_new(group,  TFRECORDOUT)\n",
    "    writer.write(tf_example.SerializeToString())\n",
    "\n",
    "writer.close()\n",
    "\n",
    "TFRECORDOUT = OUTPATH + 'mining_figures.tfrecord'\n",
    "writer = tf.io.TFRecordWriter(TFRECORDOUT)\n",
    "\n",
    "shutil.copyfile(FIGID_MODEL + LABELS, OUTPATH + 'figures_label_map.pbtxt')\n",
    "figids = figures_step3[figures_step3.figid_detection_boxes.notnull()]\n",
    "\n",
    "figsgrouped = split(figids, 'figure_path')\n",
    "\n",
    "for group in figsgrouped:\n",
    "    figtf_example = create_tf_figid(group,  TFRECORDOUT)\n",
    "    writer.write(figtf_example.SerializeToString())\n",
    "\n",
    "writer.close()\n",
    "figures_step3.to_csv(CSVOUT)\n",
    "figures_clean = figures_step3[['pub_key','pub_value','figure_tmpid','HRID','detection_scores','page_imgname','pageid_raw','figid_raw','pageid_clean','figid_clean','pageinfo_raw','figure_path','page_path']]\n",
    "figures_clean.to_csv(CLEANCSVOUT)\n",
    "\n",
    "\n",
    "#Profile segmentation\n",
    "#print('Perform image segmentation')\n",
    "#run_vesselprofile_segmentation(VESSELLPATH, SEGMENTPATH, SEG_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Empty DataFrame\nColumns: [detection_boxes, detection_classes, detection_classesname, detection_scores, figid_detection_boxes, figid_detection_classes, figid_detection_classesname, figid_detection_scores, figid_detections, figid_num_detections, figid_raw, figid_regex, figure_channel, figure_height, figure_path, figure_tmpid, figure_width, level_groups_regex, page_channel, page_detections, page_height, page_imgname, page_path, page_pdfid, page_width, pageid_clean, pageid_raw, pageid_regex, pageinfo_raw, patternHRID, pub_key, pub_name, pub_value, pubfolder_path, pubpdf_path, figid_clean, HRID]\nIndex: []\n\n[0 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "print(figures_step3.head(n=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}