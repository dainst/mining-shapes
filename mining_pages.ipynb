{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.9 64-bit",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from distutils.version import StrictVersion\n",
    "import pytesseract\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "from mining_pages_utils.image_ocr_utils import load_page, cut_image, ocr_pre_processing, ocr_post_processing_pageid, cut_image_savetemp, cut_image_figid, clean_figid\n",
    "from mining_pages_utils.dataframe_utils import get_page_labelmap_as_df, get_figid_labelmap_as_df, extract_page_detections, extract_page_detections_new,unfold_pagedetections, page_detections_toframe\n",
    "from mining_pages_utils.dataframe_utils import filter_best_page_detections, filter_best_vesselprofile_detections, merge_info, split, provide_pagelist, provide_pdf_path, get_pubs_and_configs, pdf_to_image\n",
    "from mining_pages_utils.json_utils import create_find_JSONL, create_constructivisttype_JSONL, create_normativtype_JSONL, create_drawing_JSONL, create_catalog_JSONL, create_trench_JSONL\n",
    "from mining_pages_utils.tensorflow_utils import create_tf_example_new, create_tf_figid, run_inference_for_page_series, run_inference_for_figure_series\n",
    "\n",
    "\n",
    "if StrictVersion(tf.version.VERSION) < StrictVersion('1.9.0'):\n",
    "    raise ImportError(\n",
    "        'Please upgrade your TensorFlow installation to v1.9.* or later!')\n",
    "\n",
    "INPUTDIRECTORY = '/home/images/apply' \n",
    "GRAPH = '/frozen_inference_graph.pb'\n",
    "LABELS = '/label_map.pbtxt'\n",
    "PAGE_MODEL = '/home/models/inference_graph_mining_pages_v8'\n",
    "FIGID_MODEL = '/home/models/inference_graph_figureid_v1'\n",
    "SEG_MODEL = '/home/models/shape_segmentation/train_colab_20200610.h5'\n",
    "OUTPATH = '/home/images/OUTPUT/'\n",
    "VESSELLPATH = OUTPATH + 'vesselprofiles/'\n",
    "SEGMENTPATH = OUTPATH + 'segmented_profiles/'\n",
    "CSVOUT = OUTPATH + 'mining_pages_allinfo.csv'\n",
    "CLEANCSVOUT = OUTPATH + 'mining_pages_clean.csv'\n",
    "\n",
    "classlist = ['pageid', 'pageinfo']\n",
    "figureclasslist = ['vesselprofilefigure']\n",
    "figureidclasslist = ['figureid']\n",
    "pageid_config = r'--psm 6 -c load_system_dawg=0 load_freq_dawg=0'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "publist = get_pubs_and_configs(INPUTDIRECTORY)\n",
    "pdflist = provide_pdf_path(publist)\n",
    "pdflistv2 = pdflist.apply(pdf_to_image, axis=1)\n",
    "pagelist = provide_pagelist(pdflistv2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/images/apply/ZenonID_000009465/Hayes_1972.pdf0002-142.png\n",
      "/home/images/apply/ZenonID_000009465/Hayes_1972.pdf0002-158.png\n",
      "/home/images/apply/ZenonID_000009465/Hayes_1972.pdf0003-332.png\n",
      "/home/images/apply/ZenonID_000009465/Hayes_1972.pdf0003-333.png\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'figid_detection_classes'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-126615dacb51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mall_detections_step2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpage_detections_toframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_detections_step1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mpage_category_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_page_labelmap_as_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPAGE_MODEL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mLABELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mall_detections_step2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_detections_step2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_category_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figid_detection_classes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   7957\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7958\u001b[0m             \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7959\u001b[0;31m             \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7960\u001b[0m         )\n\u001b[1;32m   7961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     )\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_rkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1561\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1563\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'figid_detection_classes'"
     ]
    }
   ],
   "source": [
    "\n",
    "for path in [VESSELLPATH, SEGMENTPATH]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.io.gfile.GFile(PAGE_MODEL + GRAPH, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "\n",
    "with detection_graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        # Get handles to input and output tensors\n",
    "        ops = tf.get_default_graph().get_operations()\n",
    "        all_tensor_names = {\n",
    "            output.name for op in ops for output in op.outputs}\n",
    "        tensor_dict = {}\n",
    "        for key in [\n",
    "            'num_detections', 'detection_boxes', 'detection_scores',\n",
    "            'detection_classes', 'detection_masks'\n",
    "        ]:\n",
    "            tensor_name = key + ':0'\n",
    "            if tensor_name in all_tensor_names:\n",
    "                tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "                    tensor_name)\n",
    "\n",
    "        all_detections_step1 = pd.DataFrame()\n",
    "\n",
    "        for index, row in pagelist.iterrows():\n",
    "\n",
    "            img = load_page(row)\n",
    "            result = run_inference_for_page_series(img, tensor_dict, sess)\n",
    "            del img\n",
    "            result.drop(\"page_imgnp\", inplace=True)\n",
    "            all_detections_step1 = all_detections_step1.append(result)\n",
    "\n",
    "#all_detections_step2, keylist =unfold_pagedetections(all_detections_step1)\n",
    "#all_detections_step22 = extract_page_detections(all_detections_step2, keylist, category_index=get_page_labelmap_as_df(PAGE_MODEL + LABELS))\n",
    "all_detections_step2 = page_detections_toframe(all_detections_step1)\n",
    "page_category_index = get_page_labelmap_as_df(PAGE_MODEL + LABELS)\n",
    "all_detections_step2 = all_detections_step2.merge(page_category_index, on=['detection_classes'], how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'all_detections_step2' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a83175f8698f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbestpages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_best_page_detections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_detections_step2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasslist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowest_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpageid_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#perform ocr page number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbestpages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_detections_step2' is not defined"
     ]
    }
   ],
   "source": [
    "bestpages = filter_best_page_detections(all_detections_step2, classlist, lowest_score=0.7)\n",
    "pageid_raw = pd.DataFrame()\n",
    "\n",
    "#perform ocr page number\n",
    "for index, row in bestpages.iterrows():\n",
    "    img = cut_image(row)\n",
    "    img2 = ocr_pre_processing(img)\n",
    "    result = pytesseract.image_to_string(img2, config=pageid_config)\n",
    "    row['newinfo'] = result\n",
    "    pageid_raw = pageid_raw.append(row)\n",
    "all_detections_step3 = merge_info(all_detections_step2, pageid_raw)\n",
    "all_detections_step3 = all_detections_step3.apply(ocr_post_processing_pageid, axis=1)\n",
    "\n",
    "figures = filter_best_vesselprofile_detections(all_detections_step3, figureclasslist,lowest_score= 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#detect figure id\n",
    "detection_figureid_graph = tf.Graph()\n",
    "with detection_figureid_graph.as_default():\n",
    "    od_graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.io.gfile.GFile(FIGID_MODEL + GRAPH, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "\n",
    "with detection_figureid_graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        # Get handles to input and output tensors\n",
    "        ops = tf.get_default_graph().get_operations()\n",
    "        all_tensor_names = {\n",
    "            output.name for op in ops for output in op.outputs}\n",
    "\n",
    "        tensor_dict = {}\n",
    "        for key in [\n",
    "            'num_detections', 'detection_boxes', 'detection_scores',\n",
    "            'detection_classes', 'detection_masks'\n",
    "        ]:\n",
    "            tensor_name = key + ':0'\n",
    "            if tensor_name in all_tensor_names:\n",
    "                tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "                    tensor_name)\n",
    "\n",
    "        figures_step1 = pd.DataFrame()\n",
    "\n",
    "        for index, row in figures.iterrows():\n",
    "            img = cut_image_savetemp(row, VESSELLPATH)\n",
    "            result = run_inference_for_figure_series(\n",
    "                img, tensor_dict, sess)\n",
    "            result.drop(\"figure_imgnp\", inplace=True)\n",
    "            figures_step1 = figures_step1.append(result)\n",
    "\n",
    "\n",
    "figid_category_index = get_figid_labelmap_as_df(FIGID_MODEL + LABELS)\n",
    "figid_detections = figures_step1.apply(\n",
    "    extract_detections_figureidv2, axis=1)\n",
    "figures_step2 = figid_detections.merge(\n",
    "    figid_category_index, on=['figid_detection_classes'], how='left')\n",
    "\n",
    "#perform ocr figid\n",
    "figures_step3 = pd.DataFrame()\n",
    "for index, row in figures_step2.iterrows():\n",
    "    img = cut_image_figid(row)\n",
    "    img2 = ocr_pre_processing(img)\n",
    "    row['figid_raw'] = pytesseract.image_to_string(img2, config=pageid_config)\n",
    "    figures_step3 = figures_step3.append(row)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(OUTPATH + 'catalogs.jsonl', 'w') as f:\n",
    "    pubs = figures_step3[['pub_key', 'pub_value']].drop_duplicates()\n",
    "    pubs.apply(create_catalog_JSONL, file=f, axis=1)\n",
    "with open(OUTPATH + 'trenches.jsonl', 'w') as f:\n",
    "    pubs = figures_step3[['pub_key', 'pub_value']].drop_duplicates()\n",
    "    pubs.apply(create_trench_JSONL, file=f, axis=1)\n",
    "with open(OUTPATH + 'types.jsonl', 'w') as f:\n",
    "    figures_step3.apply(create_constructivisttype_JSONL, file=f, axis=1)\n",
    "with open(OUTPATH + 'types_standalone.jsonl', 'w') as f:\n",
    "    figures_step3.apply(create_normativtype_JSONL, file=f, axis=1)\n",
    "with open(OUTPATH + 'finds.jsonl', 'w') as f:\n",
    "    figures_step3.apply(create_find_JSONL, file=f, axis=1)\n",
    "with open(OUTPATH + 'drawings.jsonl', 'w') as f:\n",
    "    figures_step3.apply(create_drawing_JSONL, file=f, axis=1)\n",
    "\n",
    "\n",
    "TFRECORDOUT = OUTPATH + 'mining_pages.tfrecord'\n",
    "writer = tf.io.TFRecordWriter(TFRECORDOUT)\n",
    "\n",
    "shutil.copyfile(PAGE_MODEL + LABELS, OUTPATH + 'pages_label_map.pbtxt')\n",
    "\n",
    "mining_pages_detections = figures_step3.append(bestpages)\n",
    "grouped = split(mining_pages_detections, 'page_path')\n",
    "\n",
    "for group in grouped:\n",
    "    tf_example = create_tf_example_new(group,  TFRECORDOUT)\n",
    "    writer.write(tf_example.SerializeToString())\n",
    "\n",
    "writer.close()\n",
    "\n",
    "#TFRECORDOUT = OUTPATH + 'mining_figures.tfrecord'\n",
    "#writer = tf.io.TFRecordWriter(TFRECORDOUT)\n",
    "\n",
    "#shutil.copyfile(FIGID_MODEL + LABELS, OUTPATH + 'figures_label_map.pbtxt')\n",
    "#figids = figures_step3[figures_step3.figid_detection_boxes.notnull()]\n",
    "\n",
    "#figsgrouped = split(figids, 'figure_path')\n",
    "\n",
    "#for group in figsgrouped:\n",
    "    #figtf_example = create_tf_figid(group,  TFRECORDOUT)\n",
    "    #writer.write(figtf_example.SerializeToString())\n",
    "\n",
    "#writer.close()\n",
    "figures_step3.to_csv(CSVOUT)\n",
    "figures_clean = figures_step3[['pub_key','pub_value','figure_tmpid','detection_scores','page_imgname','pageid_raw','figid_raw','pageinfo_raw','figure_path','page_path']]\n",
    "figures_clean.to_csv(CLEANCSVOUT)\n",
    "\n",
    "\n",
    "#Profile segmentation\n",
    "#print('Perform image segmentation')\n",
    "#run_vesselprofile_segmentation(VESSELLPATH, SEGMENTPATH, SEG_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(figures_step3.head(n=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}