{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from distutils.version import StrictVersion\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import cv2\n",
    "import pytesseract\n",
    "import shutil\n",
    "import json\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from object_detection.utils import dataset_util, label_map_util, ops\n",
    "from collections import namedtuple, OrderedDict\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "\n",
    "if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):\n",
    "  raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')\n",
    "  \n",
    "\n",
    "\n",
    "# This is needed to display the images.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from utils import visualization_utils as vis_util\n",
    "inputdirectory = '/home/images/apply'\n",
    "GRAPH = '/frozen_inference_graph.pb'\n",
    "LABELS = '/label_map.pbtxt'\n",
    "PAGE_MODEL = '/home/models/inference_graph_mining_pages_v8'\n",
    "FIGID_MODEL = '/home/models/inference_graph_figureid_v1'\n",
    "OUTPATH = '/home/images/OUTPUT/'\n",
    "\n",
    "def get_labelmap_as_df(PATH_TO_LABELS): \n",
    "    category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
    "    category_index = pd.DataFrame(category_index).T\n",
    "    category_index = category_index.rename(columns={'id':'detection_classes', 'name':'detection_classesname'})\n",
    "    return category_index\n",
    "\n",
    "def get_figid_labelmap_as_df(PATH_TO_LABELS): \n",
    "    category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
    "    category_index = pd.DataFrame(category_index).T\n",
    "    category_index = category_index.rename(columns={'id':'figid_detection_classes', 'name':'figid_detection_classesname'})\n",
    "    return category_index\n",
    "\n",
    "def provide_pagelist(inputdirectory, pagelist):\n",
    "\n",
    "    for pub_id in os.listdir(inputdirectory): \n",
    "        pub_key, pub_value = pub_id.split('_')\n",
    "        pub_path = os.path.join(inputdirectory, pub_id)\n",
    "        pub = {}\n",
    "        pub['pub_key'] = pub_key\n",
    "        pub['pub_value'] = pub_value                   \n",
    "        for page_imgname in os.listdir(pub_path) :\n",
    "            \n",
    "            if page_imgname.endswith((\".png\",\".jpg\")) and 'Thumbs' not in page_imgname :                \n",
    "                page = pub\n",
    "                page_path = os.path.join(pub_path, page_imgname)\n",
    "                page['page_imgname'] = page_imgname\n",
    "                page['page_path'] = page_path               \n",
    "                pagelist.append(page.copy())\n",
    "    return pd.DataFrame(pagelist)\n",
    "        \n",
    "        \n",
    "    \n",
    "                \n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_inference_for_single_image(image, graph):\n",
    "    \n",
    "    if 'detection_masks' in tensor_dict:\n",
    "        # The following processing is only for single image\n",
    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(\n",
    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "        # Follow the convention by adding back the batch dimension\n",
    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "            detection_masks_reframed, 0)\n",
    "    image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "    # Run inference\n",
    "    output_dict = sess.run(tensor_dict,\n",
    "                            feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "    # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "    output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "    output_dict['detection_classes'] = output_dict[\n",
    "        'detection_classes'][0].astype(np.uint8)\n",
    "    output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "    output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "    if 'detection_masks' in output_dict:\n",
    "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "    return output_dict\n",
    "\n",
    "def run_inference_for_series(series, graph):\n",
    "    image = series['page_imgnp']\n",
    "    \n",
    "    if 'detection_masks' in tensor_dict:\n",
    "        # The following processing is only for single image\n",
    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(\n",
    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "        # Follow the convention by adding back the batch dimension\n",
    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "            detection_masks_reframed, 0)\n",
    "    image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "    # Run inference\n",
    "    output_dict = sess.run(tensor_dict,\n",
    "                            feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "    # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "    output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "    output_dict['detection_classes'] = output_dict[\n",
    "        'detection_classes'][0].astype(np.uint8)\n",
    "    output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "    output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "    if 'detection_masks' in output_dict:\n",
    "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "    series['page_detections'] = output_dict\n",
    "    return series\n",
    "\n",
    "def run_inference_for_figureseries(series, graph):\n",
    "    image = series['figure_imgnp']\n",
    "    \n",
    "    if 'detection_masks' in tensor_dict:\n",
    "        # The following processing is only for single image\n",
    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(\n",
    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "        # Follow the convention by adding back the batch dimension\n",
    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "            detection_masks_reframed, 0)\n",
    "    image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "    # Run inference\n",
    "    output_dict = sess.run(tensor_dict,\n",
    "                            feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "    # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "    output_dict['figid_num_detections'] = int(output_dict['num_detections'][0])\n",
    "    del output_dict['num_detections']\n",
    "    output_dict['figid_detection_classes'] = output_dict[\n",
    "        'detection_classes'][0].astype(np.uint8)\n",
    "    del output_dict['detection_classes']\n",
    "    output_dict['figid_detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "    del output_dict['detection_boxes']\n",
    "    output_dict['figid_detection_scores'] = output_dict['detection_scores'][0]\n",
    "    del output_dict['detection_scores']\n",
    "    if 'detection_masks' in output_dict:\n",
    "        output_dict['figid_detection_masks'] = output_dict['detection_masks'][0]\n",
    "        del output_dict['detection_masks']\n",
    "    series['figid_detections'] = output_dict\n",
    "    return series\n",
    "\n",
    "\n",
    "    \n",
    "def extract_detections_pageold(page, output_dict):\n",
    "    page_detections = []\n",
    "    N = len(output_dict['detection_boxes'])    \n",
    "    for i in range(N):\n",
    "        detection = page\n",
    "        box = output_dict['detection_boxes'][i]\n",
    "        ymin, xmin, ymax, xmax = box\n",
    "        detection['bbox_xmin'] = int((xmin)*page_width)\n",
    "        detection['bbox_ymin'] = int((ymin)*page_height)\n",
    "        detection['bbox_xmax'] = int((xmax)*page_width)\n",
    "        detection['bbox_ymax'] = int((ymax)*page_height)\n",
    "        detection['detection_score'] = output_dict['detection_scores'][i]\n",
    "        detection_class = output_dict['detection_classes'][i]\n",
    "        if detection_class == 1:\n",
    "            detection['detection_class'] = 'vesselprofilefigure'\n",
    "        elif detection_class == 2:\n",
    "            detection['detection_class'] = 'pageid'            \n",
    "        elif detection_class == 3:\n",
    "            detection['detection_class'] = 'pageinfo'\n",
    "        page_detections.append(detection.copy())\n",
    "    return page_detections\n",
    "\n",
    "\n",
    "def extract_detections_page(df, category_index):\n",
    "    page_detectionsaslist = pd.DataFrame(df['page_detections'].tolist()).reindex(df.index)\n",
    "    df = pd.concat([df,page_detectionsaslist], axis=1)\n",
    "    all_detections = pd.DataFrame()\n",
    "    N = df['num_detections'].max()\n",
    "    for i in range(0,N):\n",
    "        detection = df.applymap(lambda x: x[i] if type(x)== np.ndarray else x)\n",
    "        all_detections = all_detections.append(detection)\n",
    "    \n",
    "    all_detections = all_detections.merge(category_index, on=['detection_classes'], how='left')\n",
    "    return all_detections\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_detections_figureid(df, figid_category_index):\n",
    "    figid_detections = pd.DataFrame(df['figid_detections'].tolist()).reindex(df.index)    \n",
    "    detections = figid_detections.applymap(lambda x: x[0] if type(x)== np.ndarray else x).reindex(figid_detections.index)    \n",
    "    detections = detections.merge(figid_category_index, on=['figid_detection_classes'], how='left')\n",
    "    df = pd.concat([df,detections])\n",
    "    return figid_detections\n",
    "\n",
    "\n",
    "    #df = pd.concat([df, figure_detectionsaslist], axis=1)\n",
    "    \n",
    "    \n",
    "    #all_detections = pd.DataFrame()\n",
    "    #N = 1\n",
    "    #for i in range(0,N):\n",
    "\n",
    "def extract_detections_figureidv2(df, figid_category_index):\n",
    "    figid_detectionsdict = df['figid_detections']\n",
    "    df['figid_detection_scores'] = figid_detectionsdict['figid_detection_scores'][0]\n",
    "    df['figid_detection_boxes'] = figid_detectionsdict['figid_detection_boxes'][0]\n",
    "    df['figid_detection_classes'] = figid_detectionsdict['figid_detection_classes'][0]\n",
    "    df['figid_num_detections'] = figid_detectionsdict['figid_num_detections']\n",
    "    #figid_detections = pd.DataFrame(df['figid_detections'].tolist()).reindex(df.index)    \n",
    "    #detections = figid_detections.applymap(lambda x: x[0] if type(x)== np.ndarray else x).reindex(figid_detections.index)    \n",
    "    #df = df.merge(figid_category_index, on=['figid_detection_classes'], how='left')\n",
    "    #df = pd.concat([df,detections])\n",
    "    return df\n",
    "\n",
    "\n",
    "    #df = pd.concat([df, figure_detectionsaslist], axis=1)\n",
    "    \n",
    "    \n",
    "    #all_detections = pd.DataFrame()\n",
    "    #N = 1\n",
    "    #for i in range(0,N):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def filter_bestdetections_max1(all_detections, classlist, lowest_score):\n",
    "    pageids = (all_detections[(all_detections['detection_classesname'].isin(classlist)) &\n",
    "      (all_detections['detection_scores'] >= lowest_score)])\n",
    "    bestdetections = (pageids[pageids['detection_scores'] == pageids\n",
    "                     .groupby(['pub_key','pub_value', 'page_imgname', 'detection_classesname'])['detection_scores'].transform('max')])\n",
    "    return bestdetections\n",
    "\n",
    "def filter_bestdetections(all_detections, classlist, lowest_score):\n",
    "    bestdetections = (all_detections[(all_detections['detection_classesname'].isin(classlist)) &\n",
    "      (all_detections['detection_scores'] >= lowest_score)])\n",
    "    return bestdetections\n",
    "\n",
    "def filter_bestdetections_figid(all_detections, classlist, lowest_score):\n",
    "    figids = (all_detections[(all_detections['figid_detection_classesname'].isin(classlist)) &\n",
    "      (all_detections['figid_detection_scores'] >= lowest_score)])\n",
    "    bestdetections = (figids[figids['figid_detection_scores'] == figids\n",
    "                     .groupby(['pub_key','pub_value', 'page_imgname', 'figure_tmpid'])['figid_detection_scores'].transform('max')])\n",
    "    return bestdetections\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cut_image(dataframe):\n",
    "    page_imgnp = cv2.imread(dataframe['page_path'])\n",
    "    box = dataframe['detection_boxes']\n",
    "    ymin, xmin, ymax, xmax = box\n",
    "    bbox_xmin = int((xmin)*dataframe['page_width'])\n",
    "    bbox_ymin = int((ymin)*dataframe['page_height'])\n",
    "    bbox_xmax = int((xmax)*dataframe['page_width'])\n",
    "    bbox_ymax = int((ymax)*dataframe['page_height'])\n",
    "    bbox_np = page_imgnp[bbox_ymin:bbox_ymax, bbox_xmin:bbox_xmax]\n",
    "    \n",
    "    return bbox_np\n",
    "\n",
    "def cut_image_savetemp(dataframe):\n",
    "    page_imgnp = cv2.imread(dataframe['page_path'])\n",
    "    box = dataframe['detection_boxes']\n",
    "    ymin, xmin, ymax, xmax = box\n",
    "    bbox_xmin = int((xmin)*dataframe['page_width'])\n",
    "    bbox_ymin = int((ymin)*dataframe['page_height'])\n",
    "    bbox_xmax = int((xmax)*dataframe['page_width'])\n",
    "    bbox_ymax = int((ymax)*dataframe['page_height'])\n",
    "    bbox_np = page_imgnp[bbox_ymin:bbox_ymax, bbox_xmin:bbox_xmax]\n",
    "    figure_height, figure_width, figure_channel = bbox_np.shape\n",
    "    dataframe['figure_height'] = figure_height\n",
    "    dataframe['figure_width'] = figure_width\n",
    "    dataframe['figure_channel'] = figure_channel\n",
    "    dataframe['figure_imgnp'] = bbox_np\n",
    "    dataframe['figure_tmpid'] = dataframe.name\n",
    "    dataframe['figure_path'] = OUTPATH + str(dataframe['pub_key']) + '_' + str(dataframe['pub_value']) + '_' + 'tempid' + str(dataframe['figure_tmpid']) + '.png'\n",
    "    cv2.imwrite( str(dataframe['figure_path']), bbox_np )\n",
    "    \n",
    "    return dataframe\n",
    " \n",
    "def cut_image_figid(dataframe):\n",
    "    figure_imgnp = cv2.imread(dataframe['figure_path'])\n",
    "    box = dataframe['figid_detection_boxes']\n",
    "    ymin, xmin, ymax, xmax = box\n",
    "    bbox_xmin = int((xmin)*dataframe['figure_width'])\n",
    "    bbox_ymin = int((ymin)*dataframe['figure_height'])\n",
    "    bbox_xmax = int((xmax)*dataframe['figure_width'])\n",
    "    bbox_ymax = int((ymax)*dataframe['figure_height'])\n",
    "    bbox_np = figure_imgnp[bbox_ymin:bbox_ymax, bbox_xmin:bbox_xmax]\n",
    "    \n",
    "    return bbox_np                               \n",
    " \n",
    "def ocrPreProcessing(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.medianBlur(image,5)\n",
    "    image = cv2.adaptiveThreshold(image,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\n",
    "    row, col = image.shape[:2]\n",
    "    bottom = image[row-2:row, 0:col]\n",
    "    mean = cv2.mean(bottom)[0]\n",
    "    \n",
    "    bordersize = 20\n",
    "    image = cv2.copyMakeBorder(\n",
    "        image,\n",
    "        top=bordersize,\n",
    "        bottom=bordersize,\n",
    "        left=bordersize,\n",
    "        right=bordersize,\n",
    "        borderType=cv2.BORDER_CONSTANT,\n",
    "        value=[mean, mean, mean]\n",
    "    )\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "def ocrPostProcessing_Pageid(row):\n",
    "    pageid_raw = row['pageid_raw']\n",
    "    row['pageid_int'] = [int(s) for s in pageid_raw.split() if s.isdigit()]\n",
    "    return row\n",
    "\n",
    "\n",
    "def  merge_info(all_detections, bestpages_result ):\n",
    "    for detection_classesname in bestpages_result.detection_classesname.unique():\n",
    "        #print (detection_class)\n",
    "        selected_info = bestpages_result[bestpages_result['detection_classesname'] == detection_classesname]\n",
    "        newinfo_name = detection_classesname + '_raw'\n",
    "        selected_info = selected_info.rename(columns={'newinfo' : newinfo_name })\n",
    "        all_detections = all_detections.merge(selected_info[[newinfo_name,'pub_key','pub_value','page_imgname']], on=['pub_key','pub_value','page_imgname'], how='left')\n",
    "                    \n",
    "    return all_detections\n",
    "       \n",
    "def load_figure(series):\n",
    "    figure_imgnp = cv2.imread( str(series['figure_path']))\n",
    "    figure_height, figure_width, figure_channel = figure_imgnp.shape\n",
    "    series['figure_width'] = figure_width\n",
    "    series['figure_height'] = figure_height\n",
    "    series['figure_channel'] = figure_channel\n",
    "    series['figure_imgnp'] = figure_imgnp\n",
    "        \n",
    "    return series\n",
    "\n",
    "def load_page(series):\n",
    "    page_imgnp = cv2.imread( str(series['page_path']))\n",
    "\n",
    "    page_height, page_width, page_channel = page_imgnp.shape\n",
    "    series['page_width'] = page_width\n",
    "    series['page_height'] = page_height\n",
    "    series['page_channel'] = page_channel\n",
    "    series['page_imgnp'] = page_imgnp\n",
    "        \n",
    "    return  series\n",
    "\n",
    "def class_text_to_int(row_label):\n",
    "    if row_label == 'vesselprofilefigure':\n",
    "        return 1\n",
    "    if row_label == 'pageid':\n",
    "        return 2\n",
    "    if row_label == 'pageinfo':\n",
    "        return 3\n",
    "    else:\n",
    "        None\n",
    "\n",
    "\n",
    "def split(df, group):\n",
    "    data = namedtuple('data', ['filename', 'object'])\n",
    "    gb = df.groupby(group)\n",
    "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
    "\n",
    "\n",
    "def createFIND_JSONL(df, file):   \n",
    "    FIND_template = '{\"category\":\"\",\"identifier\":\"\",\"relations\":{\"isChildOf\":\"\",\"isDepictedIn\":[],\"isInstanceOf\":[]}}'\n",
    "    FIND = json.loads(FIND_template)\n",
    "    print(FIND)\n",
    "    #print(df['figure_tmpid'])\n",
    "    FIND[\"identifier\"] = 'Find_' + str(df['figure_tmpid'])\n",
    "    FIND[\"category\"] = 'Pottery'\n",
    "    \n",
    "    #FIND[\"shortDescription\"] = str(df['figid_raw'])\n",
    "    relations = FIND[\"relations\"]\n",
    "    relations[\"isChildOf\"] = 'Findspot_refferedtoin_' + str(df['pub_key']) + '_' + str(df['pub_value'])\n",
    "    InstanceOfList = relations[\"isInstanceOf\"]\n",
    "    typename = 'Type_' + str(df['pub_key']) + '_' + str(df['pub_value']) + '_' + 'tempid' + str(df['figure_tmpid'])\n",
    "    InstanceOfList.append(typename)\n",
    "    #print(type(relations['isDepictedIn']))\n",
    "    depictedInList = relations[\"isDepictedIn\"]\n",
    "    imagename = str(df['pub_key']) + '_' + str(df['pub_value']) + '_' + 'tempid' +  str(df['figure_tmpid'] )+ '.png'\n",
    "    depictedInList.append(imagename)\n",
    "    print(type(FIND))\n",
    "    json.dump(FIND, file)\n",
    "    file.write(\"\\n\")\n",
    "  \n",
    "def createTYPE_JSONL(df, file):   \n",
    "    TYPE_template = '{\"category\":\"\",\"identifier\":\"\",\"relations\":{\"isChildOf\":\"\"}}'\n",
    "    TYPE = json.loads(TYPE_template)\n",
    "    #print(df['figure_tmpid'])\n",
    "    TYPE[\"identifier\"] = 'Type_' + str(df['pub_key']) + '_' + str(df['pub_value']) + '_' + 'tempid' + str(df['figure_tmpid'])\n",
    "    TYPE[\"category\"] = 'Type'\n",
    "    relations = TYPE[\"relations\"]\n",
    "    relations[\"isChildOf\"] = 'Catalog_' + str(df['pub_key']) + '_' + str(df['pub_value'])\n",
    "    #print(type(relations['isDepictedIn']))\n",
    "    json.dump(TYPE, file)\n",
    "    file.write(\"\\n\")\n",
    "    \n",
    "def createDRAWING_JSONL(df, file):   \n",
    "    DRAWING_template = '{\"category\":\"\",\"identifier\":\"\", \"description\":\"none\",\"literature\":[{\"quotation\":\"none\",\"zenonId\":\"\"}]}'\n",
    "    DRAWING = json.loads(DRAWING_template)\n",
    "    #print(df['figure_tmpid'])\n",
    "    DRAWING[\"identifier\"] = str(df['pub_key']) + '_' + str(df['pub_value']) + '_' + 'tempid' +  str(df['figure_tmpid'] )+ '.png'\n",
    "    DRAWING[\"category\"] = 'Drawing'\n",
    "    DRAWING[\"description\"] = 'PAGEID_RAW: ' + str(df['pageid_raw']) + 'PAGEINFO_RAW: ' + str(df['pageinfo_raw'])\n",
    "    literature = DRAWING[\"literature\"]\n",
    "    literature0 = literature[0]\n",
    "    literature0['zenonId'] = str(df['pub_key']) + '_' + str(df['pub_value'])\n",
    "    \n",
    "    literature0['quotation'] = str(df['figid_raw'])\n",
    "    if not literature0['quotation']:\n",
    "        literature0['quotation'] = 'no page detected'\n",
    "\n",
    "    #print(DRAWING(relations['isDepictedIn']))\n",
    "    json.dump(DRAWING, file)\n",
    "    file.write(\"\\n\")\n",
    "\n",
    "def createCATALOG_JSONL(df, file):\n",
    "    CATALOG_template = '{\"category\":\"\",\"identifier\":\"\",\"shortDescription\":\"In what aspects differ types in this catalog and what do they have in common?\", \"relations\":{\"isDepictedIn\":[]}}'\n",
    "    CATALOG = json.loads(CATALOG_template)\n",
    "    #print(df['figure_tmpid'])\n",
    "    CATALOG[\"identifier\"] = 'Catalog_' + str(df['pub_key']) + '_' + str(df['pub_value'])\n",
    "    relations = CATALOG[\"relations\"]\n",
    "    depictedInList = relations[\"isDepictedIn\"]\n",
    "    depictedInList.append('Catalogcover_' + str(df['pub_key']) + '_' + str(df['pub_value']) + '.png')\n",
    "    CATALOG[\"category\"] = 'TypeCatalog'\n",
    "    json.dump(CATALOG, file)\n",
    "    file.write(\"\\n\")\n",
    "\n",
    "def createTRENCH_JSONL(df, file):\n",
    "    TRENCH_template = '{\"category\":\"\",\"identifier\":\"\",\"shortDescription\":\"Where have the Objects been found?\"}'\n",
    "    TRENCH = json.loads(TRENCH_template)\n",
    "    #print(df['figure_tmpid'])\n",
    "    TRENCH[\"identifier\"] = 'Findspot_refferedtoin_' + str(df['pub_key']) + '_' + str(df['pub_value'])\n",
    "    TRENCH[\"category\"] = 'Trench'\n",
    "    json.dump(TRENCH, file)\n",
    "    file.write(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_tf_example(group, path):\n",
    "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = Image.open(encoded_jpg_io)\n",
    "    width, height = image.size\n",
    "    \n",
    "\n",
    "    filename = group.filename.encode('utf8')\n",
    "    image_format = b'jpg'\n",
    "    xmins = []\n",
    "    xmaxs = []\n",
    "    ymins = []\n",
    "    ymaxs = []\n",
    "    classes_text = []\n",
    "    classes = []\n",
    "\n",
    "    for index, row in group.object.iterrows():\n",
    "        box = row['detection_boxes']\n",
    "        ymin, xmin, ymax, xmax = box\n",
    "        xmins.append(xmin)\n",
    "        xmaxs.append(xmax)\n",
    "        ymins.append(ymin)\n",
    "        ymaxs.append(ymax)\n",
    "        classes_text.append(row['detection_classesname'].encode('utf8'))\n",
    "        classes.append(int(row['detection_classes']))\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "    return tf_example\n",
    "\n",
    "\n",
    "def create_tf_figid(group, path):\n",
    "\n",
    "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = Image.open(encoded_jpg_io)\n",
    "    width, height = image.size\n",
    "    \n",
    "    \n",
    "    filename = group.filename.encode('utf8')\n",
    "    image_format = b'png'\n",
    "    xmins = []\n",
    "    xmaxs = []\n",
    "    ymins = []\n",
    "    ymaxs = []\n",
    "    classes_text = []\n",
    "    classes = []\n",
    "\n",
    "\n",
    "\n",
    "    for index, row in group.object.iterrows():\n",
    "        \n",
    "        filename = group.filename.encode('utf8')\n",
    "        box = row['figid_detection_boxes']\n",
    "        ymin, xmin, ymax, xmax = box\n",
    "        xmins.append(xmin)\n",
    "        xmaxs.append(xmax)\n",
    "        ymins.append(ymin)\n",
    "        ymaxs.append(ymax)\n",
    "        classes_text.append(str(row['figid_detection_classesname']).encode('utf8'))\n",
    "        classes.append(int(row['figid_detection_classes']))\n",
    "\n",
    "\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "    return tf_example\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classlist= ['pageid', 'pageinfo']\n",
    "figureclasslist= ['vesselprofilefigure']\n",
    "figureidclasslist= ['figureid']\n",
    "pageid_config = r'--psm 6 -c load_system_dawg=0 load_freq_dawg=0'\n",
    "pagelist = []\n",
    "pagelist = provide_pagelist(inputdirectory, pagelist)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(PAGE_MODEL + GRAPH, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')\n",
    "  \n",
    "\n",
    "try:\n",
    "    with detection_graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "                # Get handles to input and output tensors\n",
    "                ops = tf.get_default_graph().get_operations()\n",
    "                all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "                tensor_dict = {}\n",
    "                for key in [\n",
    "                  'num_detections', 'detection_boxes', 'detection_scores',\n",
    "                  'detection_classes', 'detection_masks'\n",
    "                ]:\n",
    "                    tensor_name = key + ':0'\n",
    "                    if tensor_name in all_tensor_names:\n",
    "                        tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "                      tensor_name)\n",
    "                e = 0\n",
    "\n",
    "\n",
    "                \n",
    "                all_detections_step1 = (pagelist.apply(load_page, axis=1)\n",
    "                                                .apply(run_inference_for_series, graph='detection_graph' , axis=1)\n",
    "                                                .drop(columns='page_imgnp', axis=1)\n",
    "                                       ) \n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    " \n",
    "               \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = get_labelmap_as_df(PAGE_MODEL + LABELS)\n",
    "all_detections_step2 = extract_detections_page(all_detections_step1, category_index=category_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestpages = filter_bestdetections_max1(all_detections_step2, classlist , 0.7 )\n",
    "pageid_raw = (bestpages.apply(cut_image, axis=1)\n",
    "                         .apply(ocrPreProcessing)\n",
    "                         .apply(pytesseract.image_to_string, config=pageid_config)\n",
    "                         .rename(\"newinfo\", inplace=True)\n",
    ")\n",
    "\n",
    "bestpages_result = pd.concat([bestpages, pageid_raw], axis=1)\n",
    "all_detections_step3 = merge_info(all_detections_step2, bestpages_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "figures = filter_bestdetections(all_detections_step3, figureclasslist , 0.7 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detection_figureid_graph = tf.Graph()\n",
    "with detection_figureid_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(FIGID_MODEL + GRAPH, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "try:\n",
    "    with detection_figureid_graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "                # Get handles to input and output tensors\n",
    "                ops = tf.get_default_graph().get_operations()\n",
    "                all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "                \n",
    "                tensor_dict = {}\n",
    "                for key in [\n",
    "                  'num_detections', 'detection_boxes', 'detection_scores',\n",
    "                  'detection_classes', 'detection_masks'\n",
    "                ]:\n",
    "                    tensor_name = key + ':0'\n",
    "                    if tensor_name in all_tensor_names:\n",
    "                        tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "                      tensor_name)\n",
    "                e = 0\n",
    "                \n",
    "                \n",
    "                #print (figures_imgnp)\n",
    "                \n",
    "\n",
    "\n",
    "                figures_step1 = (figures.apply(cut_image_savetemp, axis=1)\n",
    "                                                  .apply(run_inference_for_figureseries, graph='detection_figureid_graph', axis=1)\n",
    "                                                  .drop(columns='figure_imgnp'))\n",
    "\n",
    "\n",
    "             \n",
    "                                       \n",
    "                                                  \n",
    "                \n",
    "                \n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "figid_category_index = get_figid_labelmap_as_df(FIGID_MODEL + LABELS)\n",
    "#figures_step2 = figures_step1.merge(figid_category_index, on=['figid_detection_classes'], how='left')\n",
    "#figid_detections= extract_detections_figureid(figures_step1, figid_category_index= figid_category_index)\n",
    "figid_detections= figures_step1.apply(extract_detections_figureidv2, figid_category_index= figid_category_index, axis=1)\n",
    "figures_step2 = figid_detections.merge(figid_category_index, on=['figid_detection_classes'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bestfigid = filter_bestdetections_figid(figures_step2, figureidclasslist , 0.6 )\n",
    "figid_raw = (figures_step2.apply(cut_image_figid, axis=1)\n",
    "                      .apply(ocrPreProcessing)\n",
    "                      .apply(pytesseract.image_to_string, config=pageid_config)\n",
    "                      .rename(\"newinfo\", inplace=True))\n",
    "figures_step3 = pd.concat([figures_step2, figid_raw], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n{'category': '', 'identifier': '', 'relations': {'isChildOf': '', 'isDepictedIn': [], 'isInstanceOf': []}}\n<class 'dict'>\n"
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'figid_raw'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4409\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4410\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlibindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value_at\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4411\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_at\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_at\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/util.pxd\u001b[0m in \u001b[0;36mpandas._libs.util.get_value_at\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/util.pxd\u001b[0m in \u001b[0;36mpandas._libs.util.validate_indexer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4c25db0134e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfigures_step3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreateFIND_JSONL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'drawings.jsonl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mfigures_step3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreateDRAWING_JSONL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m#for i in figures_step8.index:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#FIND = createFIND_JSONL(figures_step8[i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   6876\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6877\u001b[0m         )\n\u001b[0;32m-> 6878\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6880\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 result = libreduction.compute_reduction(\n\u001b[0;32m--> 296\u001b[0;31m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 )\n\u001b[1;32m    298\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.compute_reduction\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.Reducer.get_result\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-6ccca9ddc90b>\u001b[0m in \u001b[0;36mcreateDRAWING_JSONL\u001b[0;34m(df, file)\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0mliterature0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'zenonId'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pub_key'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pub_value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m     \u001b[0mliterature0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quotation'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figid_raw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mliterature0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quotation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mliterature0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quotation'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'no page detected'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4416\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4417\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4418\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4419\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4420\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4402\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4403\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4404\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4405\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4406\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'figid_raw'"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(OUTPATH + 'catalogs.jsonl', 'w') as f:\n",
    "    pubs = figures_step3[['pub_key','pub_value']].drop_duplicates()\n",
    "    pubs.apply(createCATALOG_JSONL, file = f, axis = 1)\n",
    "with open(OUTPATH + 'trenches.jsonl', 'w') as f:\n",
    "    pubs = figures_step3[['pub_key','pub_value']].drop_duplicates()\n",
    "    pubs.apply(createTRENCH_JSONL, file = f, axis = 1)\n",
    "with open(OUTPATH + 'types.jsonl', 'w') as f:\n",
    "    figures_step3.apply(createTYPE_JSONL, file = f, axis = 1)\n",
    "with open(OUTPATH + 'finds.jsonl', 'w') as f:\n",
    "    figures_step3.apply(createFIND_JSONL, file = f, axis = 1)\n",
    "with open(OUTPATH + 'drawings.jsonl', 'w') as f:\n",
    "    figures_step3.apply(createDRAWING_JSONL, file = f, axis = 1)\n",
    "    #for i in figures_step8.index:\n",
    "        #FIND = createFIND_JSONL(figures_step8[i])\n",
    "        #f.write(\"%s\\n\" % FIND)\n",
    "    \n",
    "\n",
    "    \n",
    "#df['json'] = df.apply(lambda x: x.to_json(), axis=1)    \n",
    "\n",
    "#with jsonlines.open(, 'w') as outfile:\n",
    "            #outfile.write(figures_jsonl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "TFRECORDOUT = OUTPATH + 'mining_pages.tfrecord'\n",
    "writer = tf.python_io.TFRecordWriter(TFRECORDOUT )\n",
    "\n",
    "shutil.copyfile(PAGE_MODEL + LABELS, OUTPATH + 'pages_label_map.pbtxt' )\n",
    "\n",
    "mining_pages_detections = figures_step3.append(bestpages)\n",
    "grouped = split(mining_pages_detections, 'page_path')\n",
    "\n",
    "for group in grouped:\n",
    "    tf_example = create_tf_example(group,  TFRECORDOUT)\n",
    "    writer.write(tf_example.SerializeToString())\n",
    "  \n",
    "writer.close()\n",
    "\n",
    "TFRECORDOUT = OUTPATH + 'mining_figures.tfrecord'\n",
    "writer = tf.python_io.TFRecordWriter(TFRECORDOUT )\n",
    "\n",
    "shutil.copyfile(FIGID_MODEL + LABELS, OUTPATH + 'figures_label_map.pbtxt' )\n",
    "#figids = figures_step3[figures_step3.figid_detection_boxes.notnull()]\n",
    "\n",
    "figsgrouped = split(figids, 'figure_path')\n",
    "\n",
    "for group in figsgrouped:\n",
    "    figtf_example = create_tf_figid(group,  TFRECORDOUT)\n",
    "    writer.write(figtf_example.SerializeToString())\n",
    "  \n",
    "writer.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit0ebbb96d7ebf4f37bf8afe85dc2be395",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}