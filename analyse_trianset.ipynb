{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import Keras\n",
    "#from datumaro.components.project import Project\n",
    "from datumaro.components.dataset import Dataset\n",
    "import datumaro.plugins.transforms as transforms\n",
    "from datumaro.components.project import Environment, Project\n",
    "from datumaro.components.operations import merge_categories, MergingStrategy\n",
    "from datumaro.components.operations import IntersectMerge\n",
    "from datumaro.components.extractor import (Importer, Extractor, Transform, DatasetItem, Bbox, AnnotationType, Label,\n",
    "    LabelCategories, PointsCategories, MaskCategories)\n",
    "import zipfile\n",
    "import shutil\n",
    "#import random\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"E:/Traindata/Trainingdata_fromCVAT/mining_pages\"\n",
    "TRAIN_PART = 0.7\n",
    "\n",
    "def provide_recorddf(path) -> pd.DataFrame:\n",
    "    list_of_files = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            row = {}\n",
    "            if filename in ['test.tfrecord', 'val.tfrecord', 'train.tfrecord']:\n",
    "                row['filename'] = filename\n",
    "                row['filepath'] = os.path.join(dirpath, filename)\n",
    "                list_of_files.append(row)\n",
    "                print(row)\n",
    "    return pd.DataFrame(list_of_files)\n",
    "\n",
    "def unzip_tfrecords(path):\n",
    "    i = 1\n",
    "    listoftfrecordfiles = []\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.zip'):\n",
    "\n",
    "                listoftfrecordfiles.append(tfrecordfiles)\n",
    "                #zip_ref.extractall(DIR)\n",
    "                i = i + 1\n",
    "    return listoftfrecordfiles\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dataset_shapes(dataset):\n",
    "    try:\n",
    "        return [x.get_shape().as_list() for x in dataset._tensors]\n",
    "    except TypeError:\n",
    "        return dataset._tensors.get_shape().as_list()\n",
    "\n",
    "def loadtfrecord(path):\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(path, compression_type=None, buffer_size=None, num_parallel_reads=None)\n",
    "    return dataset\n",
    "\n",
    "def converttolist(path):\n",
    "    records = []\n",
    "    for record in tf.data.Iterator(path):\n",
    "        records.append(record)\n",
    "    return records\n",
    "\n",
    "def traintestsplit(dataset):\n",
    "    split = 3\n",
    "    dataset_train = dataset.window(split, split + 1).flat_map(lambda ds: ds)\n",
    "    dataset_test = dataset.skip(split).window(1, split + 1).flat_map(lambda ds: ds)\n",
    "    return dataset_train , dataset_test\n",
    "\n",
    "def writetrainandtest(train,test, i):\n",
    "    test_writer = os.path.join(DIR, 'x0' +str(i) + \"_test.tfrecord\")\n",
    "    train_writer = os.path.join(DIR, 'x0' + str(i) + \"_train.tfrecord\")\n",
    "\n",
    "    writer = tf.data.experimental.TFRecordWriter(test_writer)\n",
    "    writer.write(test)\n",
    "    writer = tf.data.experimental.TFRecordWriter(train_writer)\n",
    "    writer.write(train)\n",
    "def extract_fn(data_record):\n",
    "    features = {\n",
    "        # Extract features using the keys set during creation\n",
    "        \"image/class/label\":    tf.FixedLenFeature([], tf.int64),\n",
    "        \"image/encoded\":        tf.VarLenFeature(tf.string),\n",
    "    }\n",
    "    sample = tf.parse_single_example(data_record, features)\n",
    "    label = sample['image/class/label']\n",
    "    dense = tf.sparse_tensor_to_dense(sample['image/encoded'])\n",
    "\n",
    "    # Comment it if you got an error and inspect just dense:\n",
    "    image = tf.image.decode_image(dense, dtype=tf.float32) \n",
    "\n",
    "    return dense, image, label\n",
    "def correctionsMiningPages(listoftrainsets,listoftestsets,listofvalsets):\n",
    "    merger = IntersectMerge()\n",
    "    merged_trainset = merger(listoftrainsets)\n",
    "    del listoftrainsets\n",
    "    merged_trainset = merged_trainset.transform('remap_labels', {'stampbox': 'infoframe' }, default='keep')\n",
    "    merged_trainset = merged_trainset.transform('remap_labels', {'pageid': 'figureid' }, default='keep')\n",
    "    trainset_path = os.path.join(DIR, 'trainset.tfrecord')\n",
    "    print('trainset')\n",
    "    print(merged_trainset.categories())\n",
    "    print(len(merged_trainset))\n",
    "    merged_trainset.export(trainset_path, 'tf_detection_api', save_images=True)\n",
    "    merged_testset = merger(listoftestsets)\n",
    "    del listoftestsets\n",
    "\n",
    "    merged_testset = merged_testset.transform('remap_labels', {'stampbox': 'infoframe' }, default='keep')\n",
    "    \n",
    "    merged_testset = merged_testset.transform('remap_labels', {'pageid': 'figureid' }, default='keep')\n",
    "    testset_path = os.path.join(DIR, 'testset.tfrecord')\n",
    "    print('testset')\n",
    "    print(merged_testset.categories())\n",
    "    print(len(merged_testset))\n",
    "    merged_testset.export(testset_path, 'tf_detection_api', save_images=True)\n",
    "    merged_valset = merger(listofvalsets)\n",
    "    del listofvalsets\n",
    "\n",
    "    merged_valset = merged_valset.transform('remap_labels', {'stampbox': 'infoframe' }, default='keep')\n",
    "    merged_valset = merged_valset.transform('remap_labels', {'pageid': 'figureid' }, default='keep')\n",
    "    valset_path = os.path.join(DIR, 'valset.tfrecord')\n",
    "    print('valset')\n",
    "    print(merged_valset.categories())\n",
    "    print(len(merged_valset))\n",
    "    merged_valset.export(valset_path, 'tf_detection_api', save_images=True)\n",
    "\n",
    "def splitEachRecord(listoftfrecordfiles):\n",
    "    listoftrainsets = []\n",
    "    listoftestsets = []\n",
    "    listofvalsets = []\n",
    "    for record in listoftfrecordfiles:\n",
    "        print(record['name'])\n",
    "        \n",
    "        \n",
    "        dataset= Dataset.import_from(record['tfrpath'], 'tf_detection_api')\n",
    "        #cleanset = dataset.select(lambda item: len(item.annotations) <= 2)\n",
    "        #for item in cleanset:\n",
    "            #print(item.annotations)\n",
    "\n",
    "\n",
    "        print(record['tfrpath'])\n",
    "        if 'task_mining_figures_zenonid_000066595_300pages-2021_02_16_12_47_10-tfrecord 1.0' in record['name']:\n",
    "            print(len(dataset))\n",
    "            dataset=dataset.select(lambda item: len(item.annotations) != 0)\n",
    "            print(len(dataset))\n",
    "        if 'task_mining_pages_zenonid_000147534_selectedpages-2021_02_23_16_00_02-tfrecord 1.0' in record['name']:\n",
    "            dataset = dataset.transform('remap_labels', {'stampfigure':'stampfigure','vesselprofilefigure': 'vesselprofilefigure', 'pageid':'pageid', 'pageinfo':'pageinfo', 'vesselimage':'vesselimage' }, default='delete')\n",
    "        if 'task_mining_pages_zenonid_000009465-2020_11_10_09_30_39-tfrecord 1.0' in record['name']:\n",
    "            dataset = dataset.transform('remap_labels', {'stampfigure':'stampfigure','vesselprofilefigure': 'vesselprofilefigure', 'pageid':'pageid', 'pageinfo':'pageinfo', 'vesselimage':'vesselimage' }, default='delete')\n",
    "        if 'task_zenonid_000267012_and_zenonid_001508696-2020_11_20_09_34_56-tfrecord 1.0' in record['name']:\n",
    "            dataset = dataset.transform('remap_labels', {'stampfigure':'stampfigure','vesselprofilefigure': 'vesselprofilefigure', 'pageid':'pageid', 'pageinfo':'pageinfo', 'vesselimage':'vesselimage' }, default='delete')\n",
    "        if 'task_zenonid_001344933_and_zenonid_001346932-2020_11_12_14_13_46-tfrecord 1.0' in record['name']:\n",
    "            dataset = dataset.transform('remap_labels', {'stampfigure':'stampfigure','vesselprofilefigure': 'vesselprofilefigure', 'pageid':'pageid', 'pageinfo':'pageinfo', 'vesselimage':'vesselimage' }, default='delete')\n",
    "\n",
    "        splitted = transforms.RandomSplit(dataset, splits=[('train', 0.60), ('test', 0.15), ('val', 0.25)])\n",
    "        train = splitted.get_subset('train')\n",
    "        trainset = Dataset.from_extractors(train)\n",
    "        test = splitted.get_subset('test')\n",
    "        testset = Dataset.from_extractors(test)\n",
    "        val = splitted.get_subset('val')\n",
    "        valset = Dataset.from_extractors(val)\n",
    "        #train, test = splitter\n",
    "        listoftrainsets.append(trainset)\n",
    "        listoftestsets.append(testset)\n",
    "        listofvalsets.append(valset)\n",
    "\n",
    "    return listoftrainsets,listoftestsets,listofvalsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofrecords = provide_recorddf(DIR) \n",
    "print(listofrecords.iloc[0]['filepath'])\n",
    "onimageList = []\n",
    "annotationList = []\n",
    "for index,record in listofrecords.iterrows():\n",
    "    dataset= Dataset.import_from(record['filepath'], 'tf_detection_api')\n",
    "    for data in dataset:\n",
    "        dict = vars(data)\n",
    "        onimageList.append(dict)\n",
    "        for anno in dict['annotations']:\n",
    "            annotationList.append(anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(len(annotationList))\n",
    "    #print(record['filepath'], len(dataset))\n",
    "#dataset = Environment().make_importer('E:/Traindata/Trainingdata_fromCVAT/mining_figures/Bonifay2004quick/testset.tfrecord').make_dataset()\n",
    "# load a Datumaro project\n",
    "#project = Project.load('E:/Traindata/Trainingdata_fromCVAT/mining_pages/datumaroproject')\n",
    "\n",
    "\n",
    "\n",
    "#dataset = Dataset.from_extractors(dataset1, dataset2)\n",
    "#ms = MergingStrategy()\n",
    "\n",
    "#print(merged.categories())\n",
    "#print(dataset2.categories())\n",
    "#mergecats = ms.merge([dataset1.categories(), dataset2.categories()])\n",
    "#categories = dataset.transform.categories()\n",
    "#print(categories.get(AnnotationType.label))\n",
    "#newdataset = dataset.transform('remap_labels', {'pageid': 'dog' }, default='delete')\n",
    "#print(mergecats)\n",
    "#for key, value in categories.items() :\n",
    "    #print(type(categories[key]))\n",
    "#print(dataset[0])\n",
    "#for item in dataset:\n",
    "    #print(item)\n",
    "\n",
    "\n",
    "\n",
    "  #print(item.annotations)\n",
    "#dataset.export(cocofile, 'coco')\n",
    "# create a dataset\n",
    "#dataset = project.make_dataset()\n",
    "\n",
    "# keep only annotated images\n",
    "#dataset.select(lambda item: len(item.annotations) != 0)\n",
    "#print(dataset.categories())\n",
    "\n",
    "# change dataset labels\n",
    "#dataset.transform('remap_labels', {'0': '666'}, default = 'delete')\n",
    "\n",
    "# iterate over dataset elements\n",
    "#i = 0\n",
    "#for item in dataset:\n",
    "    #if i < 20:\n",
    "        #for a in item.annotations:\n",
    "            #print(a.label)\n",
    "    #i = i +1 \n",
    "\n",
    "#{<AnnotationType.label: 1>: LabelCategories(attributes=set(), items=[LabelCategories.Category(name='pageid', parent='', attributes=set()), LabelCategories.Category(name='pageinfo', parent='', attributes=set()), LabelCategories.Category(name='vesselprofilefigure', parent='', attributes=set()), LabelCategories.Category(name='vesselimage', parent='', attributes=set()), LabelCategories.Category(name='infoframe', parent='', attributes=set()), LabelCategories.Category(name='stampfigure', parent='', attributes=set())], _indices={'pageid': 0, 'pageinfo': 1, 'vesselprofilefigure': 2, 'vesselimage': 3, 'infoframe': 4, 'stampfigure': 5})}\n",
    "    #for i in item.annotations:\n",
    "        #print(i)\n",
    "  #print(item.id, item.annotations)\n",
    "\n",
    "# export the resulting dataset in COCO format\n",
    "#dataset.export('dst/dir', 'coco')\n",
    "#dataset = tf.data.TFRecordDataset(filename, compression_type=None, buffer_size=None, num_parallel_reads=None)\n",
    "#for example in tf.compat.v1.python_io.tf_record_iterator(filename):\n",
    "    #print(tf.train.Example.FromString(example))\n",
    "#for element in dataset:\n",
    "    #print(elemen)\n",
    "#print(list(dataset.as_numpy_iterator()))\n",
    "\n",
    "#dataset = dataset.map(extract_fn)\n",
    "#iterator = dataset.make_one_shot_iterator()\n",
    "#next_element = iterator.get_next()\n",
    "\n",
    "\n",
    "#tf.enable_eager_execution()\n",
    "#for images, labels in dataset.take(1):  # only take first element of dataset\n",
    "    #numpy_images = images.numpy()\n",
    "    #numpy_labels = labels.numpy()\n",
    "    #print(numpy_labels)\n",
    "    #image = image.reshape(IMAGE_SHAPE)\n",
    "\n",
    "\n",
    "#for tfrecords in listoftfrecordfiles:\n",
    "    #dataset=loadtfrecord(tfrecords['tfrpath'])\n",
    "    #for element in dataset.as_numpy_iterator():\n",
    "\n",
    "    #train, test = traintestsplit(record)\n",
    "    #writetrainandtest(train,test, tfrecords['id'])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45cb965d259566d63a3e50626ed20823c3a963e454ecba058204a235f5436453"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('tensorflow2gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
